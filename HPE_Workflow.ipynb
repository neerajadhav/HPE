{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import random\n",
        "import zipfile\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "try:\n",
        "    from pycocotools.coco import COCO\n",
        "except ImportError:\n",
        "    COCO = None\n",
        "\n",
        "try:\n",
        "    import wget\n",
        "except ImportError:\n",
        "    wget = None\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "train_images_dir = None\n",
        "val_images_dir = None\n",
        "annotations_dir = None\n",
        "train_ann_file = None\n",
        "val_ann_file = None\n",
        "data_dir = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human Pose Estimation Pipeline\n",
        "This notebook implements a complete pipeline for Human Pose Estimation using the COCO dataset with proper data augmentation and keypoint transformations.\n",
        "\n",
        "## Pipeline Phases:\n",
        "1. **Environment Setup & Dependencies**\n",
        "2. **Data Download & Preparation** \n",
        "3. **Dataset Implementation**\n",
        "4. **Data Augmentation with Keypoint Transformations**\n",
        "5. **Visualization & Validation**\n",
        "6. **Pipeline Execution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Environment Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    global COCO, wget, drive\n",
        "    \n",
        "    packages_to_install = []\n",
        "    \n",
        "    if COCO is None:\n",
        "        packages_to_install.append('pycocotools')\n",
        "    if wget is None:\n",
        "        packages_to_install.append('wget')\n",
        "    packages_to_install.extend(['opencv-python', 'matplotlib'])\n",
        "    \n",
        "    if packages_to_install:\n",
        "        for package in packages_to_install:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "    \n",
        "    if COCO is None:\n",
        "        from pycocotools.coco import COCO\n",
        "        globals()['COCO'] = COCO\n",
        "        \n",
        "    if wget is None:\n",
        "        import wget\n",
        "        globals()['wget'] = wget\n",
        "    \n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    \n",
        "    return True\n",
        "\n",
        "setup_success = setup_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Data Download & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data_directories():\n",
        "    global data_dir, train_images_dir, val_images_dir, annotations_dir, train_ann_file, val_ann_file\n",
        "    \n",
        "    if drive is not None:\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            data_dir = \"/content/drive/MyDrive/ColabDataset/coco_full\"\n",
        "        except:\n",
        "            data_dir = \"./coco_data\"\n",
        "    else:\n",
        "        data_dir = \"./coco_data\"\n",
        "    \n",
        "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    urls = {\n",
        "        \"train2017\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
        "        \"val2017\": \"http://images.cocodataset.org/zips/val2017.zip\", \n",
        "        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "    }\n",
        "    \n",
        "    def download_and_extract(name, url):\n",
        "        zip_path = os.path.join(data_dir, f\"{name}.zip\")\n",
        "        extract_path = os.path.join(data_dir, name)\n",
        "\n",
        "        if os.path.exists(extract_path) and os.listdir(extract_path):\n",
        "            return True\n",
        "\n",
        "        if not os.path.exists(zip_path):\n",
        "            if wget is not None:\n",
        "                wget.download(url, zip_path)\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_dir)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    success = True\n",
        "    for key, url in urls.items():\n",
        "        if not download_and_extract(key, url):\n",
        "            success = False\n",
        "            break\n",
        "    \n",
        "    if success:\n",
        "        train_images_dir = os.path.join(data_dir, \"train2017\")\n",
        "        val_images_dir = os.path.join(data_dir, \"val2017\")\n",
        "        annotations_dir = os.path.join(data_dir, \"annotations\")\n",
        "        train_ann_file = os.path.join(annotations_dir, \"person_keypoints_train2017.json\")\n",
        "        val_ann_file = os.path.join(annotations_dir, \"person_keypoints_val2017.json\")\n",
        "        \n",
        "        critical_files = [train_ann_file, val_ann_file]\n",
        "        missing_files = [f for f in critical_files if not os.path.exists(f)]\n",
        "        \n",
        "        if missing_files:\n",
        "            return False\n",
        "    \n",
        "    return success\n",
        "\n",
        "data_success = prepare_data_directories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Dataset Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset_classes():\n",
        "    global COCODataset, get_dataloader\n",
        "    \n",
        "    class COCODataset(Dataset):\n",
        "        def __init__(self, annotation_file, image_dir, transform=None):\n",
        "            if not os.path.exists(annotation_file):\n",
        "                raise FileNotFoundError(f\"Annotation file {annotation_file} not found.\")\n",
        "            if not os.path.exists(image_dir):\n",
        "                raise FileNotFoundError(f\"Image directory {image_dir} not found.\")\n",
        "\n",
        "            self.coco = COCO(annotation_file)\n",
        "            self.image_dir = image_dir\n",
        "            self.transform = transform\n",
        "\n",
        "            all_img_ids = self.coco.getImgIds(catIds=[1])\n",
        "            self.img_ids = []\n",
        "            for img_id in all_img_ids:\n",
        "                img_info = self.coco.loadImgs(img_id)[0]\n",
        "                img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "                if os.path.exists(img_path):\n",
        "                    self.img_ids.append(img_id)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.img_ids)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_id = self.img_ids[idx]\n",
        "            img_info = self.coco.loadImgs(img_id)[0]\n",
        "            img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "            img = cv2.imread(img_path)\n",
        "            img = img[:, :, ::-1]\n",
        "\n",
        "            orig_h, orig_w = img.shape[:2]\n",
        "            img_resized = cv2.resize(img, (256, 256))\n",
        "\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[1])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "            keypoints_list = []\n",
        "            for ann in anns:\n",
        "                if 'keypoints' in ann and np.sum(ann['keypoints']) > 0:\n",
        "                    kps = np.array(ann['keypoints']).reshape(-1, 3)[:, :2]\n",
        "                    keypoints_list.append(kps)\n",
        "            keypoints = keypoints_list[0] if keypoints_list else np.zeros((17, 2))\n",
        "\n",
        "            keypoints[:, 0] = keypoints[:, 0] * 256 / orig_w\n",
        "            keypoints[:, 1] = keypoints[:, 1] * 256 / orig_h\n",
        "\n",
        "            img_tensor = torch.tensor(img_resized).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img_tensor, keypoints = self.transform(img_tensor, keypoints)\n",
        "            \n",
        "            return img_tensor, keypoints\n",
        "    \n",
        "    def get_dataloader(annotation_file, image_dir, batch_size=4, transform=None):\n",
        "        dataset = COCODataset(annotation_file, image_dir, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    return COCODataset, get_dataloader\n",
        "\n",
        "try:\n",
        "    COCODataset, get_dataloader = create_dataset_classes()\n",
        "    dataset_success = True\n",
        "except Exception as e:\n",
        "    dataset_success = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Data Augmentation with Keypoint Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_augmentation_transforms():\n",
        "    global KeypointTransform, DebugTransform\n",
        "    \n",
        "    class KeypointTransform:\n",
        "        def __init__(self, flip_prob=0.5, rotation_degrees=10, brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1):\n",
        "            self.flip_prob = flip_prob\n",
        "            self.rotation_degrees = rotation_degrees\n",
        "            self.brightness = brightness\n",
        "            self.contrast = contrast\n",
        "            self.saturation = saturation\n",
        "            self.hue = hue\n",
        "        \n",
        "        def __call__(self, image, keypoints):\n",
        "            if torch.is_tensor(image):\n",
        "                image = TF.to_pil_image(image)\n",
        "            \n",
        "            img_width, img_height = image.size\n",
        "            was_flipped = False\n",
        "            rotation_angle = 0\n",
        "            \n",
        "            if random.random() < self.flip_prob:\n",
        "                image = TF.hflip(image)\n",
        "                was_flipped = True\n",
        "            \n",
        "            if self.rotation_degrees > 0:\n",
        "                rotation_angle = random.uniform(-self.rotation_degrees, self.rotation_degrees)\n",
        "                image = TF.rotate(image, rotation_angle)\n",
        "            \n",
        "            if self.brightness > 0 or self.contrast > 0 or self.saturation > 0 or self.hue > 0:\n",
        "                image = TF.adjust_brightness(image, 1 + random.uniform(-self.brightness, self.brightness))\n",
        "                image = TF.adjust_contrast(image, 1 + random.uniform(-self.contrast, self.contrast))\n",
        "                image = TF.adjust_saturation(image, 1 + random.uniform(-self.saturation, self.saturation))\n",
        "                image = TF.adjust_hue(image, random.uniform(-self.hue, self.hue))\n",
        "            \n",
        "            image = TF.to_tensor(image)\n",
        "            transformed_keypoints = keypoints.copy()\n",
        "            \n",
        "            if was_flipped:\n",
        "                transformed_keypoints[:, 0] = img_width - 1 - transformed_keypoints[:, 0]\n",
        "                swap_pairs = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n",
        "                for left_idx, right_idx in swap_pairs:\n",
        "                    if left_idx < len(transformed_keypoints) and right_idx < len(transformed_keypoints):\n",
        "                        transformed_keypoints[[left_idx, right_idx]] = transformed_keypoints[[right_idx, left_idx]]\n",
        "            \n",
        "            if abs(rotation_angle) > 0:\n",
        "                center_x, center_y = img_width / 2, img_height / 2\n",
        "                angle_rad = np.radians(-rotation_angle)\n",
        "                cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)\n",
        "                \n",
        "                x_centered = transformed_keypoints[:, 0] - center_x\n",
        "                y_centered = transformed_keypoints[:, 1] - center_y\n",
        "                \n",
        "                x_rotated = x_centered * cos_angle - y_centered * sin_angle\n",
        "                y_rotated = x_centered * sin_angle + y_centered * cos_angle\n",
        "                \n",
        "                transformed_keypoints[:, 0] = x_rotated + center_x\n",
        "                transformed_keypoints[:, 1] = y_rotated + center_y\n",
        "                \n",
        "                transformed_keypoints[:, 0] = np.clip(transformed_keypoints[:, 0], 0, img_width - 1)\n",
        "                transformed_keypoints[:, 1] = np.clip(transformed_keypoints[:, 1], 0, img_height - 1)\n",
        "            \n",
        "            return image, transformed_keypoints\n",
        "    \n",
        "    class DebugTransform:\n",
        "        def __init__(self, flip=False, rotation=0):\n",
        "            self.flip = flip\n",
        "            self.rotation = rotation\n",
        "        \n",
        "        def __call__(self, image, keypoints):\n",
        "            if torch.is_tensor(image):\n",
        "                image = TF.to_pil_image(image)\n",
        "            \n",
        "            img_width, img_height = image.size\n",
        "            transformed_keypoints = keypoints.copy()\n",
        "            \n",
        "            if self.flip:\n",
        "                image = TF.hflip(image)\n",
        "                transformed_keypoints[:, 0] = img_width - 1 - transformed_keypoints[:, 0]\n",
        "                swap_pairs = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n",
        "                for left_idx, right_idx in swap_pairs:\n",
        "                    if left_idx < len(transformed_keypoints) and right_idx < len(transformed_keypoints):\n",
        "                        transformed_keypoints[[left_idx, right_idx]] = transformed_keypoints[[right_idx, left_idx]]\n",
        "            \n",
        "            if self.rotation != 0:\n",
        "                image = TF.rotate(image, self.rotation)\n",
        "                center_x, center_y = img_width / 2, img_height / 2\n",
        "                angle_rad = np.radians(-self.rotation)\n",
        "                cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)\n",
        "                \n",
        "                x_centered = transformed_keypoints[:, 0] - center_x\n",
        "                y_centered = transformed_keypoints[:, 1] - center_y\n",
        "                \n",
        "                x_rotated = x_centered * cos_angle - y_centered * sin_angle\n",
        "                y_rotated = x_centered * sin_angle + y_centered * cos_angle\n",
        "                \n",
        "                transformed_keypoints[:, 0] = x_rotated + center_x\n",
        "                transformed_keypoints[:, 1] = y_rotated + center_y\n",
        "                \n",
        "                transformed_keypoints[:, 0] = np.clip(transformed_keypoints[:, 0], 0, img_width - 1)\n",
        "                transformed_keypoints[:, 1] = np.clip(transformed_keypoints[:, 1], 0, img_height - 1)\n",
        "            \n",
        "            return TF.to_tensor(image), transformed_keypoints\n",
        "    \n",
        "    return KeypointTransform, DebugTransform\n",
        "\n",
        "try:\n",
        "    KeypointTransform, DebugTransform = create_augmentation_transforms()\n",
        "    transform_success = True\n",
        "except Exception as e:\n",
        "    transform_success = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Visualization & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_visualization_functions():\n",
        "    global visualize_basic_batch, visualize_augmentation_comparison, visualize_debug_transformations\n",
        "    \n",
        "    def visualize_basic_batch(data_loader, title=\"Dataset Samples\"):\n",
        "        images, keypoints_batch = next(iter(data_loader))\n",
        "        fig, axes = plt.subplots(1, len(images), figsize=(16, 4))\n",
        "        \n",
        "        if len(images) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for i, img in enumerate(images):\n",
        "            img_np = img.permute(1, 2, 0).numpy()\n",
        "            axes[i].imshow(img_np)\n",
        "            axes[i].scatter(keypoints_batch[i][:, 0], keypoints_batch[i][:, 1], \n",
        "                          c='red', s=40, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "            axes[i].set_title(f'{title} {i+1}')\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def visualize_augmentation_comparison():\n",
        "        custom_transform = KeypointTransform(\n",
        "            flip_prob=1.0, rotation_degrees=20, brightness=0.2, \n",
        "            contrast=0.2, saturation=0.2, hue=0.05\n",
        "        )\n",
        "\n",
        "        no_transform_loader = get_dataloader(val_ann_file, val_images_dir, batch_size=2, transform=None)\n",
        "        transform_loader = get_dataloader(val_ann_file, val_images_dir, batch_size=2, transform=custom_transform)\n",
        "\n",
        "        torch.manual_seed(42)\n",
        "        original_images, original_keypoints = next(iter(no_transform_loader))\n",
        "\n",
        "        torch.manual_seed(42)\n",
        "        transformed_images, transformed_keypoints = next(iter(transform_loader))\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "        for i in range(2):\n",
        "            img_np = original_images[i].permute(1, 2, 0).numpy()\n",
        "            axes[0, i].imshow(img_np)\n",
        "            axes[0, i].scatter(original_keypoints[i][:, 0], original_keypoints[i][:, 1], \n",
        "                             c='red', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "            axes[0, i].set_title(f'Original {i+1}', fontsize=14)\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "        for i in range(2):\n",
        "            img_np = transformed_images[i].permute(1, 2, 0).numpy()\n",
        "            axes[1, i].imshow(img_np)\n",
        "            axes[1, i].scatter(transformed_keypoints[i][:, 0], transformed_keypoints[i][:, 1], \n",
        "                             c='blue', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "            axes[1, i].set_title(f'Transformed {i+1}', fontsize=14)\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def visualize_debug_transformations():\n",
        "        transforms_to_test = [\n",
        "            (\"Original\", None),\n",
        "            (\"Horizontal Flip\", DebugTransform(flip=True)),\n",
        "            (\"Rotation 15°\", DebugTransform(rotation=15)),\n",
        "            (\"Rotation 30°\", DebugTransform(rotation=30)),\n",
        "            (\"Rotation -15°\", DebugTransform(rotation=-15)),\n",
        "            (\"Flip + Rotate 15°\", DebugTransform(flip=True, rotation=15)),\n",
        "            (\"Flip + Rotate 30°\", DebugTransform(flip=True, rotation=30)),\n",
        "            (\"Flip + Rotate -15°\", DebugTransform(flip=True, rotation=-15))\n",
        "        ]\n",
        "\n",
        "        sample_dataset = COCODataset(val_ann_file, val_images_dir, transform=None)\n",
        "        sample_img1, sample_kpts1 = sample_dataset[0]\n",
        "        sample_img2, sample_kpts2 = sample_dataset[1]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "        for i in range(4):\n",
        "            title, transform = transforms_to_test[i]\n",
        "            if transform is None:\n",
        "                img_show = sample_img1\n",
        "                kpts_show = sample_kpts1\n",
        "            else:\n",
        "                img_show, kpts_show = transform(sample_img1, sample_kpts1)\n",
        "            \n",
        "            img_np = img_show.permute(1, 2, 0).numpy()\n",
        "            axes[0, i].imshow(img_np)\n",
        "            axes[0, i].scatter(kpts_show[:, 0], kpts_show[:, 1], c='red', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "            axes[0, i].set_title(f'{title} - Sample 1', fontsize=12)\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "        for i in range(4):\n",
        "            title, transform = transforms_to_test[i + 4]\n",
        "            if transform is None:\n",
        "                img_show = sample_img2\n",
        "                kpts_show = sample_kpts2\n",
        "            else:\n",
        "                img_show, kpts_show = transform(sample_img2, sample_kpts2)\n",
        "            \n",
        "            img_np = img_show.permute(1, 2, 0).numpy()\n",
        "            axes[1, i].imshow(img_np)\n",
        "            axes[1, i].scatter(kpts_show[:, 0], kpts_show[:, 1], c='blue', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "            axes[1, i].set_title(f'{title} - Sample 2', fontsize=12)\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return visualize_basic_batch, visualize_augmentation_comparison, visualize_debug_transformations\n",
        "\n",
        "try:\n",
        "    vis_basic, vis_aug, vis_debug = create_visualization_functions()\n",
        "    visualization_success = True\n",
        "except Exception as e:\n",
        "    visualization_success = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_pipeline():\n",
        "    if not all([setup_success, data_success, dataset_success, transform_success, visualization_success]):\n",
        "        return False\n",
        "    \n",
        "    train_loader = get_dataloader(train_ann_file, train_images_dir, batch_size=4)\n",
        "    val_loader = get_dataloader(val_ann_file, val_images_dir, batch_size=4)\n",
        "    \n",
        "    visualize_basic_batch(val_loader, \"Original Dataset\")\n",
        "    visualize_augmentation_comparison()\n",
        "    visualize_debug_transformations()\n",
        "    \n",
        "    return True\n",
        "\n",
        "pipeline_success = execute_pipeline()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
