{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5ff6d36"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Colab setup\n",
        "# -----------------------------\n",
        "!pip install -q pycocotools opencv-python wget matplotlib\n",
        "\n",
        "import os, wget, zipfile\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaaa542e",
        "outputId": "c448fbef-f1df-4c4e-d502-294452b51ff7"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Define paths and URLs\n",
        "# -----------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set data directory in your Google Drive\n",
        "data_dir = \"/content/drive/MyDrive/ColabDataset/coco_full\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "urls = {\n",
        "    \"train2017\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
        "    \"val2017\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
        "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aed2a8c1",
        "outputId": "d2433d61-5ffb-4b1a-9887-d091df94a71d"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Download and extract function\n",
        "# -----------------------------\n",
        "def download_and_extract(name, url):\n",
        "    zip_path = os.path.join(data_dir, f\"{name}.zip\")\n",
        "    extract_path = os.path.join(data_dir, name)\n",
        "\n",
        "    if os.path.exists(extract_path):\n",
        "        print(f\"{name} already exists, skipping download/extraction.\")\n",
        "        return\n",
        "\n",
        "    # Download if zip doesn't exist\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Downloading {name}...\")\n",
        "        try:\n",
        "            wget.download(url, zip_path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to download {name}: {e}\")\n",
        "\n",
        "    # Extract\n",
        "    print(f\"\\nExtracting {name}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "    except zipfile.BadZipFile:\n",
        "        raise RuntimeError(f\"Bad zip file for {name}. Delete {zip_path} and retry.\")\n",
        "    print(f\"{name} ready.\")\n",
        "\n",
        "# Download all datasets safely\n",
        "for key, url in urls.items():\n",
        "    download_and_extract(key, url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8af7d0b3"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 3. Paths\n",
        "# -----------------------------\n",
        "train_images_dir = os.path.join(data_dir, \"train2017\")\n",
        "val_images_dir   = os.path.join(data_dir, \"val2017\")\n",
        "annotations_dir  = os.path.join(data_dir, \"annotations\")\n",
        "train_ann_file = os.path.join(annotations_dir, \"person_keypoints_train2017.json\")\n",
        "val_ann_file   = os.path.join(annotations_dir, \"person_keypoints_val2017.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccb74bae"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 4. COCO Dataset class (human-only + existing images)\n",
        "# -----------------------------\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, annotation_file, image_dir, transform=None):\n",
        "        if not os.path.exists(annotation_file):\n",
        "            raise FileNotFoundError(f\"Annotation file {annotation_file} not found.\")\n",
        "        if not os.path.exists(image_dir):\n",
        "            raise FileNotFoundError(f\"Image directory {image_dir} not found.\")\n",
        "\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Filter: only human images that exist\n",
        "        all_img_ids = self.coco.getImgIds(catIds=[1])\n",
        "        self.img_ids = []\n",
        "        for img_id in all_img_ids:\n",
        "            img_info = self.coco.loadImgs(img_id)[0]\n",
        "            img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "            if os.path.exists(img_path):\n",
        "                self.img_ids.append(img_id)\n",
        "\n",
        "        print(f\"Loaded {len(self.img_ids)} human images from {image_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = img[:, :, ::-1]  # BGR -> RGB\n",
        "\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "        img_resized = cv2.resize(img, (256, 256))\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[1])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        keypoints_list = []\n",
        "        for ann in anns:\n",
        "            if 'keypoints' in ann and np.sum(ann['keypoints']) > 0:\n",
        "                kps = np.array(ann['keypoints']).reshape(-1, 3)[:, :2]\n",
        "                keypoints_list.append(kps)\n",
        "        keypoints = keypoints_list[0] if keypoints_list else np.zeros((17, 2))\n",
        "\n",
        "        # Scale keypoints to 256x256\n",
        "        keypoints[:, 0] = keypoints[:, 0] * 256 / orig_w\n",
        "        keypoints[:, 1] = keypoints[:, 1] * 256 / orig_h\n",
        "\n",
        "        img_tensor = torch.tensor(img_resized).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # Apply transform if provided\n",
        "        if self.transform is not None:\n",
        "            img_tensor, keypoints = self.transform(img_tensor, keypoints)\n",
        "        \n",
        "        return img_tensor, keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b75b8710",
        "outputId": "c56e9399-4b92-4bb0-e608-33c80dbb45ad"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 5. DataLoader helper (human-only)\n",
        "# -----------------------------\n",
        "def get_dataloader(annotation_file, image_dir, batch_size=4, transform=None):\n",
        "    dataset = COCODataset(annotation_file, image_dir, transform=transform)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Example usage\n",
        "train_loader = get_dataloader(train_ann_file, train_images_dir, batch_size=4)\n",
        "val_loader   = get_dataloader(val_ann_file, val_images_dir, batch_size=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 4.5. Custom Transform Class for Keypoints\n",
        "# -----------------------------\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class KeypointTransform:\n",
        "    def __init__(self, flip_prob=0.5, rotation_degrees=10, brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1):\n",
        "        self.flip_prob = flip_prob\n",
        "        self.rotation_degrees = rotation_degrees\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "        self.saturation = saturation\n",
        "        self.hue = hue\n",
        "    \n",
        "    def __call__(self, image, keypoints):\n",
        "        # Convert to PIL if it's a tensor\n",
        "        if torch.is_tensor(image):\n",
        "            image = TF.to_pil_image(image)\n",
        "        \n",
        "        # Get image dimensions\n",
        "        img_width, img_height = image.size  # PIL format: (width, height)\n",
        "        \n",
        "        # Track transformations\n",
        "        was_flipped = False\n",
        "        rotation_angle = 0\n",
        "        \n",
        "        # Random horizontal flip\n",
        "        if random.random() < self.flip_prob:\n",
        "            image = TF.hflip(image)\n",
        "            was_flipped = True\n",
        "        \n",
        "        # Random rotation\n",
        "        if self.rotation_degrees > 0:\n",
        "            rotation_angle = random.uniform(-self.rotation_degrees, self.rotation_degrees)\n",
        "            image = TF.rotate(image, rotation_angle)\n",
        "        \n",
        "        # Color jitter\n",
        "        if self.brightness > 0 or self.contrast > 0 or self.saturation > 0 or self.hue > 0:\n",
        "            image = TF.adjust_brightness(image, 1 + random.uniform(-self.brightness, self.brightness))\n",
        "            image = TF.adjust_contrast(image, 1 + random.uniform(-self.contrast, self.contrast))\n",
        "            image = TF.adjust_saturation(image, 1 + random.uniform(-self.saturation, self.saturation))\n",
        "            image = TF.adjust_hue(image, random.uniform(-self.hue, self.hue))\n",
        "        \n",
        "        # Convert back to tensor\n",
        "        image = TF.to_tensor(image)\n",
        "        \n",
        "        # Transform keypoints\n",
        "        transformed_keypoints = keypoints.copy()\n",
        "        \n",
        "        # Apply horizontal flip to keypoints\n",
        "        if was_flipped:\n",
        "            # Flip x-coordinates: new_x = width - old_x\n",
        "            transformed_keypoints[:, 0] = img_width - 1 - transformed_keypoints[:, 0]\n",
        "            \n",
        "            # For human pose, we also need to swap left/right keypoints\n",
        "            # COCO keypoint order: 0=nose, 1=left_eye, 2=right_eye, 3=left_ear, 4=right_ear,\n",
        "            # 5=left_shoulder, 6=right_shoulder, 7=left_elbow, 8=right_elbow,\n",
        "            # 9=left_wrist, 10=right_wrist, 11=left_hip, 12=right_hip,\n",
        "            # 13=left_knee, 14=right_knee, 15=left_ankle, 16=right_ankle\n",
        "            swap_pairs = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n",
        "            for left_idx, right_idx in swap_pairs:\n",
        "                if left_idx < len(transformed_keypoints) and right_idx < len(transformed_keypoints):\n",
        "                    transformed_keypoints[[left_idx, right_idx]] = transformed_keypoints[[right_idx, left_idx]]\n",
        "        \n",
        "        # Apply rotation to keypoints (around center)\n",
        "        if abs(rotation_angle) > 0:\n",
        "            center_x, center_y = img_width / 2, img_height / 2\n",
        "            angle_rad = np.radians(-rotation_angle)  # Negative because image rotation is clockwise\n",
        "            cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)\n",
        "            \n",
        "            # Translate to origin, rotate, translate back\n",
        "            x_centered = transformed_keypoints[:, 0] - center_x\n",
        "            y_centered = transformed_keypoints[:, 1] - center_y\n",
        "            \n",
        "            x_rotated = x_centered * cos_angle - y_centered * sin_angle\n",
        "            y_rotated = x_centered * sin_angle + y_centered * cos_angle\n",
        "            \n",
        "            transformed_keypoints[:, 0] = x_rotated + center_x\n",
        "            transformed_keypoints[:, 1] = y_rotated + center_y\n",
        "            \n",
        "            # Clip to image bounds\n",
        "            transformed_keypoints[:, 0] = np.clip(transformed_keypoints[:, 0], 0, img_width - 1)\n",
        "            transformed_keypoints[:, 1] = np.clip(transformed_keypoints[:, 1], 0, img_height - 1)\n",
        "        \n",
        "        return image, transformed_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "f1777fb0",
        "outputId": "d2551efc-88e3-4813-bccb-9fb28a98163f"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 6. Visualize a batch\n",
        "# -----------------------------\n",
        "images, keypoints_batch = next(iter(val_loader))\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(16, 4))\n",
        "\n",
        "for i, img in enumerate(images):\n",
        "    img_np = img.permute(1, 2, 0).numpy()\n",
        "    axes[i].imshow(img_np)\n",
        "    axes[i].scatter(keypoints_batch[i][:, 0], keypoints_batch[i][:, 1], c='r', s=40)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Train images: {len(train_loader.dataset)}, Val images: {len(val_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 7. Transform images and visualize with proper keypoint handling\n",
        "# -----------------------------\n",
        "\n",
        "# Create custom transform that handles both images and keypoints\n",
        "custom_transform = KeypointTransform(\n",
        "    flip_prob=1.0,  # Always flip for demonstration\n",
        "    rotation_degrees=20,  # Reduced rotation for better visualization\n",
        "    brightness=0.2,  # Reduced brightness changes\n",
        "    contrast=0.2,\n",
        "    saturation=0.2,\n",
        "    hue=0.05\n",
        ")\n",
        "\n",
        "# Also create a no-transform version for comparison\n",
        "no_transform_loader = get_dataloader(val_ann_file, val_images_dir, batch_size=2, transform=None)\n",
        "transform_loader = get_dataloader(val_ann_file, val_images_dir, batch_size=2, transform=custom_transform)\n",
        "\n",
        "# Get the same images (set seed for consistency)\n",
        "torch.manual_seed(42)\n",
        "original_images, original_keypoints = next(iter(no_transform_loader))\n",
        "\n",
        "torch.manual_seed(42)\n",
        "transformed_images, transformed_keypoints = next(iter(transform_loader))\n",
        "\n",
        "# Visualize original vs transformed\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Original images (top row)\n",
        "for i in range(2):\n",
        "    img_np = original_images[i].permute(1, 2, 0).numpy()\n",
        "    axes[0, i].imshow(img_np)\n",
        "    axes[0, i].scatter(original_keypoints[i][:, 0], original_keypoints[i][:, 1], c='red', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "    axes[0, i].set_title(f'Original {i+1}', fontsize=14)\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Transformed images (bottom row)\n",
        "for i in range(2):\n",
        "    img_np = transformed_images[i].permute(1, 2, 0).numpy()\n",
        "    axes[1, i].imshow(img_np)\n",
        "    axes[1, i].scatter(transformed_keypoints[i][:, 0], transformed_keypoints[i][:, 1], c='blue', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "    axes[1, i].set_title(f'Transformed {i+1}', fontsize=14)\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üî¥ Red dots: Original keypoints\")\n",
        "print(\"üîµ Blue dots: Transformed keypoints\")\n",
        "print(\"‚úÖ The keypoints should now accurately follow the image transformations!\")\n",
        "print(\"üìù Note: Horizontal flip also swaps left/right body parts (e.g., left eye ‚Üî right eye)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 7.5. Debug keypoint transformation\n",
        "# -----------------------------\n",
        "# Let's create a simple test to verify keypoint transformations work correctly\n",
        "\n",
        "# Create a deterministic transform for testing\n",
        "class DebugTransform:\n",
        "    def __init__(self, flip=False, rotation=0):\n",
        "        self.flip = flip\n",
        "        self.rotation = rotation\n",
        "    \n",
        "    def __call__(self, image, keypoints):\n",
        "        if torch.is_tensor(image):\n",
        "            image = TF.to_pil_image(image)\n",
        "        \n",
        "        img_width, img_height = image.size\n",
        "        transformed_keypoints = keypoints.copy()\n",
        "        \n",
        "        if self.flip:\n",
        "            image = TF.hflip(image)\n",
        "            # Flip x-coordinates\n",
        "            transformed_keypoints[:, 0] = img_width - 1 - transformed_keypoints[:, 0]\n",
        "            # Swap left/right keypoints\n",
        "            swap_pairs = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n",
        "            for left_idx, right_idx in swap_pairs:\n",
        "                if left_idx < len(transformed_keypoints) and right_idx < len(transformed_keypoints):\n",
        "                    transformed_keypoints[[left_idx, right_idx]] = transformed_keypoints[[right_idx, left_idx]]\n",
        "        \n",
        "        if self.rotation != 0:\n",
        "            image = TF.rotate(image, self.rotation)\n",
        "            center_x, center_y = img_width / 2, img_height / 2\n",
        "            angle_rad = np.radians(-self.rotation)\n",
        "            cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)\n",
        "            \n",
        "            x_centered = transformed_keypoints[:, 0] - center_x\n",
        "            y_centered = transformed_keypoints[:, 1] - center_y\n",
        "            \n",
        "            x_rotated = x_centered * cos_angle - y_centered * sin_angle\n",
        "            y_rotated = x_centered * sin_angle + y_centered * cos_angle\n",
        "            \n",
        "            transformed_keypoints[:, 0] = x_rotated + center_x\n",
        "            transformed_keypoints[:, 1] = y_rotated + center_y\n",
        "            \n",
        "            transformed_keypoints[:, 0] = np.clip(transformed_keypoints[:, 0], 0, img_width - 1)\n",
        "            transformed_keypoints[:, 1] = np.clip(transformed_keypoints[:, 1], 0, img_height - 1)\n",
        "        \n",
        "        return TF.to_tensor(image), transformed_keypoints\n",
        "\n",
        "# Test different transformations - 8 total (4 columns x 2 rows)\n",
        "transforms_to_test = [\n",
        "    (\"Original\", None),\n",
        "    (\"Horizontal Flip\", DebugTransform(flip=True)),\n",
        "    (\"Rotation 15¬∞\", DebugTransform(rotation=15)),\n",
        "    (\"Rotation 30¬∞\", DebugTransform(rotation=30)),\n",
        "    (\"Rotation -15¬∞\", DebugTransform(rotation=-15)),\n",
        "    (\"Flip + Rotate 15¬∞\", DebugTransform(flip=True, rotation=15)),\n",
        "    (\"Flip + Rotate 30¬∞\", DebugTransform(flip=True, rotation=30)),\n",
        "    (\"Flip + Rotate -15¬∞\", DebugTransform(flip=True, rotation=-15))\n",
        "]\n",
        "\n",
        "# Get two different samples\n",
        "sample_dataset = COCODataset(val_ann_file, val_images_dir, transform=None)\n",
        "sample_img1, sample_kpts1 = sample_dataset[0]\n",
        "sample_img2, sample_kpts2 = sample_dataset[1]\n",
        "\n",
        "# Create 4 columns x 2 rows visualization\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "# First row - Sample 1\n",
        "for i in range(4):\n",
        "    title, transform = transforms_to_test[i]\n",
        "    if transform is None:\n",
        "        img_show = sample_img1\n",
        "        kpts_show = sample_kpts1\n",
        "    else:\n",
        "        img_show, kpts_show = transform(sample_img1, sample_kpts1)\n",
        "    \n",
        "    img_np = img_show.permute(1, 2, 0).numpy()\n",
        "    axes[0, i].imshow(img_np)\n",
        "    axes[0, i].scatter(kpts_show[:, 0], kpts_show[:, 1], c='red', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "    axes[0, i].set_title(f'{title} - Sample 1', fontsize=12)\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Second row - Sample 2\n",
        "for i in range(4):\n",
        "    title, transform = transforms_to_test[i + 4]\n",
        "    if transform is None:\n",
        "        img_show = sample_img2\n",
        "        kpts_show = sample_kpts2\n",
        "    else:\n",
        "        img_show, kpts_show = transform(sample_img2, sample_kpts2)\n",
        "    \n",
        "    img_np = img_show.permute(1, 2, 0).numpy()\n",
        "    axes[1, i].imshow(img_np)\n",
        "    axes[1, i].scatter(kpts_show[:, 0], kpts_show[:, 1], c='blue', s=50, alpha=0.8, edgecolors='white', linewidth=1)\n",
        "    axes[1, i].set_title(f'{title} - Sample 2', fontsize=12)\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üî¥ Red dots: Keypoints on Sample 1 (top row)\")\n",
        "print(\"üîµ Blue dots: Keypoints on Sample 2 (bottom row)\")\n",
        "print(\"‚úÖ This shows how keypoints accurately follow different transformations\")\n",
        "print(\"üìù Notice how flips swap left/right body parts and rotations preserve relative positions\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "-1.-1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
